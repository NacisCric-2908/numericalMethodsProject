\documentclass[12pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage{array}
\usepackage{float}
\geometry{margin=1in}

\title{\textbf{Newton-Raphson Multivariable: Solución Interactiva de Sistemas No Lineales}}
\author{Juan Diego Grajales Castillo - 20221020128 \\ Julian David Celis Giraldo - 20222020041 \\ Cristian Santiago Lopez Cadena - 20222020027 \\ Johan Camilo Gomez Blanco - 20222020069 \\ Universidad Distrital Francisco José de Caldas \\ Asignatura: Métodos Numéricos (2025-3)}
\date{\today}

\begin{document}

\maketitle

\section*{Resumen}
Este documento presenta una implementación interactiva del método de Newton-Raphson para resolver sistemas de ecuaciones no lineales. Se desarrolló una aplicación de escritorio en Python con interfaz gráfica (Tkinter) que permite ingresar ecuaciones, aproximaciones iniciales y parámetros de convergencia, mostrando paso a paso cada iteración del método. Se incluye la teoría subyacente, ejemplos detallados y el código fuente completo. El proyecto demuestra la utilidad del método numérico en la resolución de problemas multivariables complejos.

\section{Introducción}
Los sistemas de ecuaciones no lineales son frecuentes en ingeniería, física y ciencias computacionales. Su solución analítica a menudo es imposible, por lo que se recurre a métodos numéricos iterativos. Entre ellos, el \textbf{método de Newton-Raphson multivariable} destaca por su rapidez de convergencia cuadrática cerca de la raíz. Sin embargo, su implementación práctica requiere el cálculo del Jacobiano y una cuidadosa selección del punto inicial.

El propósito de este proyecto es \textbf{diseñar e implementar una herramienta interactiva} que aplique el método de Newton-Raphson a sistemas de cualquier tamaño, mostrando el proceso iterativo de forma clara y didáctica. La herramienta también servirá como material educativo para estudiantes de métodos numéricos.

\section{Desarrollo Teórico}
\subsection{Método de Newton-Raphson para Sistemas No Lineales}
Dado un sistema de $n$ ecuaciones con $n$ incógnitas:

\[
\mathbf{F}(\mathbf{x}) = 
\begin{cases}
f_1(x_1, x_2, \dots, x_n) = 0 \\
f_2(x_1, x_2, \dots, x_n) = 0 \\
\vdots \\
f_n(x_1, x_2, \dots, x_n) = 0
\end{cases}
\]

donde $\mathbf{x} = [x_1, x_2, \dots, x_n]^T$, el método de Newton-Raphson genera una sucesión de aproximaciones $\mathbf{x}^{(k)}$ mediante la fórmula iterativa:

\[
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \mathbf{J}^{-1}(\mathbf{x}^{(k)}) \cdot \mathbf{F}(\mathbf{x}^{(k)})
\]

donde $\mathbf{J}(\mathbf{x})$ es la matriz Jacobiana del sistema:

\[
\mathbf{J}(\mathbf{x}) = 
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \dots & \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \dots & \frac{\partial f_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial x_1} & \frac{\partial f_n}{\partial x_2} & \dots & \frac{\partial f_n}{\partial x_n}
\end{bmatrix}
\]

En la práctica, se resuelve el sistema lineal:

\[
\mathbf{J}(\mathbf{x}^{(k)}) \cdot \Delta\mathbf{x}^{(k)} = -\mathbf{F}(\mathbf{x}^{(k)})
\]

y luego se actualiza:

\[
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \Delta\mathbf{x}^{(k)}
\]

El criterio de parada se basa en la norma infinito del incremento:

\[
\|\Delta\mathbf{x}^{(k)}\|_\infty < \varepsilon
\]

o en el número máximo de iteraciones permitidas.

\subsection{Consideraciones de Implementación}
\begin{itemize}
    \item \textbf{Cálculo simbólico del Jacobiano}: Se utiliza la librería \texttt{sympy} para derivar automáticamente.
    \item \textbf{Selección del punto inicial}: Crítica para la convergencia.
    \item \textbf{Manejo de singularidades}: Si $\mathbf{J}$ es singular, el método falla.
    \item \textbf{Funciones matemáticas soportadas}: $\sen$, $\cos$, $\exp$, $\log$, $\sqrt{\phantom{1}}$, $\pi$, e, etc.
\end{itemize}

\section{Ejemplos y Problemas Resueltos}
%====================Ejemplo Julian=======================
\subsection{Ejemplo 1: Sistema 2x2}
Resolver mediante Newton--Raphson el sistema:
\[
\begin{cases}
f_1(x,y)=x^2 + y^2 - 10 = 0, \\[6pt]
f_2(x,y)=x^2 - y - 1 = 0.
\end{cases}
\]

\subsubsection*{1. Definición del vector de funciones}
\[
\mathbf{f}(x,y)=
\begin{pmatrix}
x^2 + y^2 - 10 \\[6pt]
x^2 - y - 1
\end{pmatrix}.
\]

\subsubsection*{2. Jacobiano}
\[
J(x,y)=
\begin{pmatrix}
\frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y}\\[6pt]
\frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y}
\end{pmatrix}
=
\begin{pmatrix}
2x & 2y \\[6pt]
2x & -1
\end{pmatrix}.
\]

\subsubsection*{3. Fórmula de Newton}
\[
\mathbf{u}_{k+1}=
\mathbf{u}_k
-
J(x_k,y_k)^{-1}\,
\mathbf{f}(x_k,y_k).
\]

\subsubsection*{4. Iteración inicial}
Tomamos como aproximación inicial:
\[
\mathbf{u}_0=(x_0,y_0)=(2,2).
\]

\[
\mathbf{f}(2,2)=
\begin{pmatrix}
2^2 + 2^2 - 10\\[6pt]
2^2 - 2 - 1
\end{pmatrix}
=
\begin{pmatrix}
-2\\[6pt]
1
\end{pmatrix}.
\]

Jacobiano en $(2,2)$:
\[
J(2,2)=
\begin{pmatrix}
4 & 4\\[6pt]
4 & -1
\end{pmatrix}.
\]

\subsubsection*{5. Inversa del Jacobiano}
\[
\det(J)=4(-1)-4\cdot 4=-4-16=-20.
\]

\[
J^{-1}=\frac{1}{-20}
\begin{pmatrix}
-1 & -4 \\[6pt]
-4 & 4
\end{pmatrix}.
\]

\subsubsection*{6. Cálculo de la corrección}
\[
J^{-1}\mathbf{f}(2,2)
=\frac{1}{-20}
\begin{pmatrix}
-1 & -4 \\[6pt]
-4 & 4
\end{pmatrix}
\begin{pmatrix}
-2\\[6pt]
1
\end{pmatrix}.
\]

Multiplicando:
\[
\begin{pmatrix}
(-1)(-2) + (-4)(1) \\[6pt]
(-4)(-2) + 4(1)
\end{pmatrix}
=
\begin{pmatrix}
2 - 4 \\[6pt]
8 + 4
\end{pmatrix}
=
\begin{pmatrix}
-2 \\[6pt]
12
\end{pmatrix}.
\]

\[
J^{-1}\mathbf{f}(2,2)
=
\frac{1}{-20}
\begin{pmatrix}
-2\\[6pt]
12
\end{pmatrix}
=
\begin{pmatrix}
0.1\\[6pt]
-0.6
\end{pmatrix}.
\]

\subsubsection*{7. Actualización}
\[
\mathbf{u}_1
=\mathbf{u}_0 - 
J^{-1}\mathbf{f}(\mathbf{u}_0)
=
\begin{pmatrix}
2\\[6pt]
2
\end{pmatrix}
-
\begin{pmatrix}
0.1\\[6pt]
-0.6
\end{pmatrix}
=
\begin{pmatrix}
1.9\\[6pt]
2.6
\end{pmatrix}.
\]

\subsubsection*{8. Aproximación después de una iteración}
\[
\boxed{(x_1,y_1)\approx(1.9,\; 2.6)}
\]
\subsubsection*{9. Segunda iteración (partiendo de \(\mathbf{u}_1=(1.9,\,2.6)\))}

Evaluamos \(\mathbf{f}\) en \((1.9,2.6)\):
\[
\mathbf{f}(1.9,2.6)=
\begin{pmatrix}
1.9^2 + 2.6^2 - 10 \\[6pt]
1.9^2 - 2.6 - 1
\end{pmatrix}
=
\begin{pmatrix}
0.37 \\[6pt]
0.01
\end{pmatrix}.
\]

Jacobian en \((1.9,2.6)\):
\[
J(1.9,2.6)=
\begin{pmatrix}
2(1.9) & 2(2.6) \\[6pt]
2(1.9) & -1
\end{pmatrix}
=
\begin{pmatrix}
3.8 & 5.2 \\[6pt]
3.8 & -1
\end{pmatrix}.
\]

Determinante:
\[
\det J = 3.8(-1) - 5.2(3.8) = -23.56.
\]

Inversa (usando la adjunta / determinante):
\[
J(1.9,2.6)^{-1}=\frac{1}{-23.56}
\begin{pmatrix}
-1 & -5.2 \\[6pt]
-3.8 & 3.8
\end{pmatrix}
=
\begin{pmatrix}
0.0424448217317487 & 0.2207130730050934 \\[6pt]
0.1612903225806452 & -0.1612903225806452
\end{pmatrix}.
\]

Corrección \(\delta = J^{-1}\mathbf{f}(1.9,2.6)\):
\[
\delta =
\begin{pmatrix}
\delta_x \\[6pt] \delta_y
\end{pmatrix}
=
\begin{pmatrix}
0.0424448217317487 & 0.2207130730050934 \\[6pt]
0.1612903225806452 & -0.1612903225806452
\end{pmatrix}
\begin{pmatrix}
0.37 \\[6pt] 0.01
\end{pmatrix}
=
\begin{pmatrix}
0.01791171477079796 \\[6pt]
0.05806451612903226
\end{pmatrix}.
\]

Actualización:
\[
\mathbf{u}_2 = \mathbf{u}_1 - \delta
=
\begin{pmatrix}
1.9 \\[6pt] 2.6
\end{pmatrix}
-
\begin{pmatrix}
0.01791171477079796 \\[6pt]
0.05806451612903226
\end{pmatrix}
=
\begin{pmatrix}
1.8820882852292020 \\[6pt]
2.5419354838709677
\end{pmatrix}.
\]

Por tanto, tras la segunda iteración se obtiene la aproximación
\[
\boxed{\mathbf{u}_2 \approx (x_2,y_2) \approx (1.8820882852292020,\;2.5419354838709677).}
\]
A continuación se repetiría el procedimiento anterior partiendo desde el resultado de la segunda iteración ${u}_2$ pero por efectos prácticos procedemos a saltarnos las iteraciones intermedias y pasamos a una de las aproximaciones más cercanas y precisas

\[
\mathbf{u}_4 \approx
\begin{pmatrix}
x_4 \\[4pt] y_4
\end{pmatrix}
=
\begin{pmatrix}
1.88185580349534513286325939158 \\[4pt]
2.54138126514911026354797637379
\end{pmatrix}.
\]

Evaluamos el vector residual \(\mathbf{f}(x_4,y_4)\):
\[
\mathbf{f}(x_4,y_4)=
\begin{pmatrix}
x_4^2 + y_4^2 - 10 \\[6pt]
x_4^2 - y_4 - 1
\end{pmatrix}
\approx
\begin{pmatrix}
3.3202007412244499\times 10^{-15} \\[6pt]
7.7123045180993946\times 10^{-16}
\end{pmatrix}.
\]

Jacobiano en \((x_4,y_4)\):
\[
J(x_4,y_4)=
\begin{pmatrix}
2x_4 & 2y_4 \\[6pt]
2x_4 & -1
\end{pmatrix}
\approx
\begin{pmatrix}
3.7637116069906903 & 5.0827625302982205 \\[6pt]
3.7637116069906903 & -1
\end{pmatrix}.
\]

La corrección \(\delta = J(x_4,y_4)^{-1}\mathbf{f}(x_4,y_4)\) resulta en valores del orden de \(10^{-16}\):
\[
\delta \approx
\begin{pmatrix}
\delta_x \\[4pt] \delta_y
\end{pmatrix}
\approx
\begin{pmatrix}
3.1625127277294937\times 10^{-16} \\[6pt]
4.1904813425118894\times 10^{-16}
\end{pmatrix}.
\]

Por tanto la actualización de Newton da
\[
\mathbf{u}_5 = \mathbf{u}_4 - \delta
\approx
\begin{pmatrix}
1.88185580349534481661198661863 \\[6pt]
2.54138126514910984449984212261
\end{pmatrix}.
\]

Vemos que la corrección es prácticamente nula (órdenes \(10^{-16}\)), por lo que la solución ha convergido numéricamente y podemos tomar como solución aproximada final:

\[
\boxed{\;(x,y)\approx(1.8818558034953448,\;2.5413812651491098)\;}.
\]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{imagenes/Grafica.png}
    \caption{Gráfica del sistema no lineal}
\end{figure}

\subsubsection*{10. Comparación del resultado obtenido anteriormente con el obtenido en el programa}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{imagenes/Ej_julian.png}
    \caption{Resultado obtenido en el software}
\end{figure}
%======================================================
%====================Ejemplo johan=====================
\subsection{Ejemplo 2: Sistema 3x3}
\subsubsection*{1. Planteamiento del sistema}

Sea el sistema no lineal $F(\mathbf{x}) = \mathbf{0}$ con $\mathbf{x} = [x,y,z]^T$:

\[
\begin{cases}
f_1(x,y,z) = x^3 + y - z - 1 = 0,\\[4pt]
f_2(x,y,z) = x + y^3 + z^2 - 3 = 0,\\[4pt]
f_3(x,y,z) = x^2 + y + z^3 - 4 = 0.
\end{cases}
\]

Usamos como punto inicial:
\[
\mathbf{x}^{(0)} = \begin{bmatrix}1 \\ 1 \\ 1\end{bmatrix}.
\]
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\linewidth]{imagenes/Ej_johan.PNG}
    \caption{Superficies del sistema no lineal y punto inicial para Newton-Raphson.}
    \label{fig:superficies}
\end{figure}

\subsubsection*{2. Matriz Jacobiana}

La matriz jacobiana es:
\[
J(x,y,z)=
\begin{bmatrix}
3x^2 & 1 & -1\\[4pt]
1 & 3y^2 & 2z\\[4pt]
2x & 1 & 3z^2
\end{bmatrix}.
\]

\subsubsection*{3. Iteraciones de Newton}

En cada iteración se resuelve:
\[
J(\mathbf{x}^{(k)})\,\Delta\mathbf{x}^{(k)} = -F(\mathbf{x}^{(k)}),
\qquad
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \Delta\mathbf{x}^{(k)}.
\]

\subsubsection*{4. Iteración 0 → 1}

Punto inicial:
\[
\mathbf{x}^{(0)}=\begin{bmatrix}1 \\ 1 \\ 1\end{bmatrix}.
\]

Evaluación:
\[
F(\mathbf{x}^{(0)})=
\begin{bmatrix}
0\\
0\\
-1
\end{bmatrix}.
\]

Jacobiana:
\[
J(\mathbf{x}^{(0)})=
\begin{bmatrix}
3 & 1 & -1\\
1 & 3 & 2\\
2 & 1 & 3
\end{bmatrix}.
\]

Se resuelve:
\[
J(\mathbf{x}^{(0)})\,\Delta\mathbf{x}^{(0)}=
\begin{bmatrix}0\\0\\1\end{bmatrix}.
\]

Solución:
\[
\Delta\mathbf{x}^{(0)}\approx
\begin{bmatrix}
0.1851851852\\[4pt]
-0.2592592593\\[4pt]
0.2962962963
\end{bmatrix}.
\]

Actualización:
\[
\mathbf{x}^{(1)}\approx
\begin{bmatrix}
1.1851851852\\
0.7407407407\\
1.2962962963
\end{bmatrix}.
\]

\subsubsection*{5. Iteración 1 → 2}

\[
\mathbf{x}^{(1)}\approx
\begin{bmatrix}
1.1851851852\\
0.7407407407\\
1.2962962963
\end{bmatrix}.
\]

Evaluación:
\[
F(\mathbf{x}^{(1)})\approx
\begin{bmatrix}
0.1092313197\\
0.2720113825\\
0.3236803336
\end{bmatrix}.
\]

Jacobiana:
\[
J(\mathbf{x}^{(1)})\approx
\begin{bmatrix}
4.2139917708 & 1 & -1\\
1 & 1.6460905269 & 2.5925925926\\
2.3703703704 & 1 & 5.0411522591
\end{bmatrix}.
\]

Solución:
\[
\Delta\mathbf{x}^{(1)}\approx
\begin{bmatrix}
-0.0125473603\\
-0.0956840599\\
-0.0393271988
\end{bmatrix}.
\]

Actualización:
\[
\mathbf{x}^{(2)}\approx
\begin{bmatrix}
1.1726378249\\
0.6450566808\\
1.2569690955
\end{bmatrix}.
\]

\subsubsection*{6. Iteración 2 → 3}

\[
\mathbf{x}^{(2)}\approx
\begin{bmatrix}
1.1726378249\\
0.6450566808\\
1.2569690955
\end{bmatrix}.
\]

Evaluación:
\[
F(\mathbf{x}^{(2)})\approx
\begin{bmatrix}
0.0005577999\\
0.0210160161\\
0.0061112768
\end{bmatrix}.
\]

Jacobiana:
\[
J(\mathbf{x}^{(2)})\approx
\begin{bmatrix}
4.1252384276 & 1 & -1\\
1 & 1.2482943690 & 2.5139381910\\
2.3452756498 & 1 & 4.7399139406
\end{bmatrix}.
\]

Solución del sistema:
\[
\Delta\mathbf{x}^{(2)}\approx
\begin{bmatrix}
0.0055918894\\
-0.0228591527\\
0.0007665413
\end{bmatrix}.
\]

Actualización:
\[
\mathbf{x}^{(3)}\approx
\begin{bmatrix}
1.1782297143\\
0.6221975281\\
1.2577356368
\end{bmatrix}.
\]

\subsubsection*{7. Iteración 3 → 4}

Punto actual (resultado de la iteración 2→3):
\[
\mathbf{x}^{(3)} \approx
\begin{bmatrix}
1.1782297143\\[4pt]
0.6221975281\\[4pt]
1.2577356368
\end{bmatrix}.
\]

Evaluación:
\[
F(\mathbf{x}^{(3)}) \approx
\begin{bmatrix}
1.10142373\times 10^{-4}\\[4pt]
9.99828575\times 10^{-4}\\[4pt]
3.34484482\times 10^{-5}
\end{bmatrix},
\qquad
\|F(\mathbf{x}^{(3)})\| \approx 1.008001\times 10^{-3}.
\]

Jacobiana en \(\mathbf{x}^{(3)}\):
\[
J(\mathbf{x}^{(3)}) \approx
\begin{bmatrix}
4.16467578 & 1 & -1\\[4pt]
1 & 1.16138929 & 2.51547127\\[4pt]
2.35645943 & 1 & 4.74569680
\end{bmatrix}.
\]

Se resuelve \(J(\mathbf{x}^{(3)})\,\Delta\mathbf{x}^{(3)} = -F(\mathbf{x}^{(3)})\) y se obtiene
\[
\Delta\mathbf{x}^{(3)} \approx
\begin{bmatrix}
3.43688714\times 10^{-4}\\[4pt]
-1.42001031\times 10^{-3}\\[4pt]
1.21508669\times 10^{-4}
\end{bmatrix},
\qquad
\|\Delta\mathbf{x}^{(3)}\| \approx 1.466054\times 10^{-3}.
\]

Actualización:
\[
\mathbf{x}^{(4)} = \mathbf{x}^{(3)} + \Delta\mathbf{x}^{(3)} \approx
\begin{bmatrix}
1.1785734010\\[4pt]
0.6207775178\\[4pt]
1.2578571455
\end{bmatrix}.
\]

Residuo en \(\mathbf{x}^{(4)}\):
\[
F(\mathbf{x}^{(4)}) \approx
\begin{bmatrix}
4.17579522\times 10^{-7}\\[4pt]
3.77574121\times 10^{-6}\\[4pt]
1.73839448\times 10^{-7}
\end{bmatrix},
\qquad
\|F(\mathbf{x}^{(4)})\| \approx 3.802738\times 10^{-6}.
\]

\subsubsection*{8. Iteración 4 → 5}

Punto actual:
\[
\mathbf{x}^{(4)} \approx
\begin{bmatrix}
1.1785734010\\[4pt]
0.6207775178\\[4pt]
1.2578571455
\end{bmatrix}.
\]

Evaluación:
\[
F(\mathbf{x}^{(4)}) \approx
\begin{bmatrix}
4.17579522\times 10^{-7}\\[4pt]
3.77574121\times 10^{-6}\\[4pt]
1.73839448\times 10^{-7}
\end{bmatrix}.
\]

Jacobiana en \(\mathbf{x}^{(4)}\) (entradas evaluadas):
\[
J(\mathbf{x}^{(4)}) \approx
\begin{bmatrix}
4.17579522 & 1 & -1\\[4pt]
1 & 1.15662940 & 2.51571430\\[4pt]
2.35714680 & 1 & 4.74598340
\end{bmatrix}.
\]

Resolviendo \(J(\mathbf{x}^{(4)})\,\Delta\mathbf{x}^{(4)} = -F(\mathbf{x}^{(4)})\) se obtiene
\[
\Delta\mathbf{x}^{(4)} \approx
\begin{bmatrix}
1.29597925\times 10^{-6}\\[4pt]
-5.36746479\times 10^{-6}\\[4pt]
4.50597428\times 10^{-7}
\end{bmatrix},
\qquad
\|\Delta\mathbf{x}^{(4)}\| \approx 5.540061\times 10^{-6}.
\]

Actualización:
\[
\mathbf{x}^{(5)} = \mathbf{x}^{(4)} + \Delta\mathbf{x}^{(4)} \approx
\begin{bmatrix}
1.1785746969\\[4pt]
0.6207720533\\[4pt]
1.2578575961
\end{bmatrix}.
\]

Residuo en \(\mathbf{x}^{(5)}\):
\[
F(\mathbf{x}^{(5)}) \approx
\begin{bmatrix}
5.93880500\times 10^{-12}\\[4pt]
5.38564748\times 10^{-11}\\[4pt]
2.44515519\times 10^{-12}
\end{bmatrix},
\qquad
\|F(\mathbf{x}^{(5)})\| \approx 5.423807\times 10^{-11}.
\]

\subsubsection*{9. Resumen actualizado (incluyendo iteraciones 4 y 5)}

\[
\begin{aligned}
\mathbf{x}^{(0)} &= [1.00000000,\;1.00000000,\;1.00000000]^T, &\|F(\mathbf{x}^{(0)})\| &= 1.00000000,\\[4pt]
\mathbf{x}^{(1)} &\approx [1.18518519,\;0.74074074,\;1.29629630]^T, &\|F(\mathbf{x}^{(1)})\| &\approx 0.43668138,\\[4pt]
\mathbf{x}^{(2)} &\approx [1.17263783,\;0.64505668,\;1.25696910]^T, &\|F(\mathbf{x}^{(2)})\| &\approx 0.02189365,\\[4pt]
\mathbf{x}^{(3)} &\approx [1.17822971,\;0.62219753,\;1.25773564]^T, &\|F(\mathbf{x}^{(3)})\| &\approx 1.008001\times 10^{-3},\\[4pt]
\mathbf{x}^{(4)} &\approx [1.17857340,\;0.62077752,\;1.25785715]^T, &\|F(\mathbf{x}^{(4)})\| &\approx 3.802738\times 10^{-6},\\[4pt]
\mathbf{x}^{(5)} &\approx [1.17857470,\;0.62077205,\;1.25785760]^T, &\|F(\mathbf{x}^{(5)})\| &\approx 5.423807\times 10^{-11}.
\end{aligned}
\]
%======================================================

%===================Ejemplo Juan Diego ================
\subsection{Ejemplo 3: Sistema 2×2}
Resolver:
\[
\begin{cases}
x^2 + y^2 - 1 = 0 \\
x - y = 0
\end{cases}
\]

\textbf{Solución analítica}: $x = y = \pm \sqrt{2}/2$.

\textbf{Solución numérica}: Con punto inicial $(0.5, 0.5)$ y tolerancia $10^{-6}$, el método converge en 4 iteraciones a $(0.70710678, 0.70710678)$.

\begin{table}[H]
\centering
\begin{tabular}{cccccc}
\toprule
Iter & $x$ & $y$ & $\|\Delta\mathbf{x}\|_\infty$ & $\|\mathbf{F}(\mathbf{x})\|_\infty$ \\
\midrule
1 & 0.62500000 & 0.62500000 & 1.250e-01 & 3.125e-01 \\
2 & 0.70312500 & 0.70312500 & 7.812e-02 & 7.812e-02 \\
3 & 0.70709229 & 0.70709229 & 3.967e-04 & 3.967e-04 \\
4 & 0.70710678 & 0.70710678 & 1.449e-05 & 1.449e-05 \\
\bottomrule
\end{tabular}
\caption{Iteraciones para el Ejemplo 1.}
\end{table}

\subsection{Ejemplo 4: Sistema 3×3}
Resolver:
\[
\begin{cases}
x^2 + y^2 + z^2 - 1 \\
x + y + z - 1 \\
x^2 + y - z
\end{cases}
\]

Con punto inicial $(-0.165956,0.440649,-0.999771)$ y tolerancia $10^{-10}$, converge en 13 iteraciones a $(0.851883856,-0.28879499,0.43691113)$.
%=======================================
%============Ejemplo Santiago===========
\subsection{Ejemplo 5: Sistema 2x2}
Resolver el sistema:
\[
\begin{cases}
x^2 + y^2 - 1 = 0, \\[6pt]
\frac{4}{9}x^2 +4 y^2 - 1 = 0.
\end{cases}
\]
con aproximacion inicial (x,y)=(1,1) y una tolerancia de $1^{-16}$ y hasta 20 iteraciones. 
En la siguinete imagen podemos observar que el programa encontro una aproximacion en tan solo 7 iteraciones.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{imagenes/Ej_Santiago.png}
    \caption{Aproximacion encontrada por el software}
\end{figure}
Como podemos observar la aproximacion obtenida es:
\[
\boxed{\;(x,y)\approx(0.91855865,\;0.39528471)\;}.
\]
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{imagenes/Grafica_Ej_santiago.png}
    \caption{Grafica del sistema no lineal con la aproximacion obtenida.}
\end{figure}
%========================================

\section{Implementación del Algoritmo}
Se desarrolló una aplicación en Python con interfaz gráfica Tkinter. El código principal se estructura en dos clases:
\begin{itemize}
    \item \texttt{NewtonSystemSolver}: Lógica del método numérico.
    \item \texttt{NewtonGUIApp}: Interfaz gráfica y control de eventos.
\end{itemize}

\subsection{Código Principal (Fragmento)}
\begin{lstlisting}[language=Python, basicstyle=\small, keywordstyle=\color{blue}]
def solve(self, functions_raw, variables_raw, initial_guess_raw, tolerance, max_iterations):
    variables = self._parse_variables(variables_raw)
    initial_guess = self._parse_initial_guess(initial_guess_raw, len(variables))
    expressions, jacobian = self._parse_functions(functions_raw, variables)
    
    symbols = sp.symbols(variables)
    f_lambda = sp.lambdify(symbols, expressions, modules="numpy")
    j_lambda = sp.lambdify(symbols, jacobian, modules="numpy")
    
    current = initial_guess.astype(float)
    iterations = []
    
    for idx in range(1, max_iterations + 1):
        f_val = np.array(f_lambda(*current), dtype=float).reshape(-1)
        j_val = np.array(j_lambda(*current), dtype=float)
        
        delta = np.linalg.solve(j_val, -f_val)
        next_guess = current + delta
        
        delta_norm = float(np.linalg.norm(delta, ord=np.inf))
        residual_norm = float(np.linalg.norm(f_val, ord=np.inf))
        
        iterations.append(IterationRecord(idx, next_guess.tolist(), delta_norm, residual_norm))
        current = next_guess
        
        if delta_norm < tolerance:
            return iterations, current, True
    
    return iterations, current, False
\end{lstlisting}

\subsection{Características de la Implementación}
\begin{itemize}
    \item \textbf{Interfaz intuitiva}: Campos para variables, ecuaciones, punto inicial, tolerancia e iteraciones.
    \item \textbf{Tabla interactiva}: Muestra cada iteración con valores, normas de incremento y residuo.
    \item \textbf{Manejo de errores}: Validación de entrada, detección de singularidades.
    \item \textbf{Flexibilidad}: Soporta cualquier número de ecuaciones/variables.
\end{itemize}

\section{Conclusiones}
El método de Newton-Raphson multivariable es una herramienta poderosa para resolver sistemas no lineales, con convergencia rápida cuando se elige un buen punto inicial. La implementación presentada automatiza el cálculo del Jacobiano y proporciona una interfaz visual que facilita el análisis del proceso iterativo.

Esta herramienta no solo resuelve problemas numéricos, sino que también sirve como recurso educativo, permitiendo a los estudiantes observar cómo cambian las aproximaciones en cada paso. Futuras mejoras podrían incluir:
\begin{itemize}
    \item Métodos híbridos (ej. Newton con backtracking).
    \item Visualización gráfica de las trayectorias iterativas en 2D.
    \item Exportación de resultados a formatos como CSV o LaTeX.
\end{itemize}

\section*{Bibliografía}
Chapra, S. C., \& Canale, R. P. (2015). \textit{Métodos numéricos para ingenieros} (7ª ed.). McGraw-Hill.

Mathews, J. H., \& Fink, K. D. (2004). \textit{Numerical methods using MATLAB} (4ª ed.). Pearson.

Press, W. H., Teukolsky, S. A., Vetterling, W. T., \& Flannery, B. P. (2007). \textit{Numerical recipes: The art of scientific computing} (3ª ed.). Cambridge University Press.

SymPy Development Team. (2023). \textit{SymPy: Python library for symbolic mathematics}. Recuperado de \url{https://www.sympy.org/}

\end{document}